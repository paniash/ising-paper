\appendix
\section{Code}

\subsection{Least Square Regression}

\onecolumn
\begin{minted}{python}
import numpy as np
import matplotlib.pyplot as plt
from sklearn import linear_model

# function to return the energy of our lattice
def energy(l):
    e = 0
    j = 1
    # our energy assumes periodic boundary conditions
    for i in range(l.size - 1):
        e += -j * l[i] * (l[i - 1] + l[i + 1])

    e += -j * l[l.size - 1] * (l[l.size - 2] + l[0])
    return e


# length of our lattice
d = 50
for n in [250, 800, 1700, 2000, 4000, 5000]:
    # defining our lattice
    l = np.random.choice([-1, 1], size=(n, d))

    # array to store the labels
    label = np.zeros(n)

    # creating the labels
    for i in range(n):
        label[i] = energy(l[i])

    # partition of our training n testing sets
    m = int(7 * n / 10)

    train = l[:m]
    test = l[m:]

    train_label = label[:m]
    test_label = label[m:]

    # data generated!!

    # setting up our ML algos
    leastsq = linear_model.LinearRegression()

    # needed for training the data: computing the outer product of S_i
    train_states = np.einsum("...i,...j->...ij", train, train)
    shape = train_states.shape

    # each row is a reshaped from a 5x5 matrix of S_i*S_j
    train_states = train_states.reshape((shape[0], shape[1] * shape[2]))

    # needed for training the data: computing the outer product of S_i
    test_states = np.einsum("...i,...j->...ij", test, test)
    shape = test_states.shape

    # each row is a reshaped from a 5x5 matrix of S_i*S_j
    test_states = test_states.reshape((shape[0], shape[1] * shape[2]))

    leastsq.fit(train_states, train_label)
    J = leastsq.coef_.reshape((shape[1], shape[2]))

    mark1 = leastsq.score(train_states, train_label) * 100
    mark2 = leastsq.score(test_states, test_label) * 100

    print("n: %s" % n, "train-test: ", m, "-", n - m)
    print("Training score: %s /100" % mark1)
    print("Testing score: %s /100" % mark2, "\n")

    plt.figure()
    plt.title("n=%s" % n)
    c = plt.imshow(
        J, vmin=-1, vmax=1, cmap="Spectral_r", interpolation="nearest", origin="lower"
    )
    plt.colorbar(c)
\end{minted}

\subsection{1D training using Lasso regression}

Here, we adopt the same \texttt{energy()} function as used in least square regression but with training done via Lasso regression.

\begin{minted}{python}
import numpy as np
import matplotlib.pyplot as plt
from sklearn import linear_model

# function to return the energy of our lattice
def energy(l):
    e = 0
    j = 1
    # our energy assumes periodic boundary conditions
    for i in range(l.size - 1):

        e += -j * l[i] * (l[i - 1] + l[i + 1])

    e += -j * l[l.size - 1] * (l[l.size - 2] + l[0])

    return e

# length of our lattice
d = 50
# no of 1d ising models
narr = np.linspace(100, 5000, 100, dtype="int")

# obtain score relation with varying number of datapoints
for n in narr:
    score_tr = []
    score_te = []

    # defining our lattice
    l = np.random.choice([-1, 1], size=(n, d))

    # array to store the labels
    label = np.zeros(n)

    # creating the labels
    for i in range(n):
        label[i] = energy(l[i])

    # partition of our training n testing sets
    m = int(7 * n / 10)

    train = l[:m]
    test = l[m:]

    train_label = label[:m]
    test_label = label[m:]

    # data generated!!

    # setting up our ML algos
    lasso = linear_model.Lasso(tol=0.01)

    # needed for training the data: computing the outer product of S_i
    train_states = np.einsum("...i,...j->...ij", train, train)
    shape = train_states.shape

    # each row is a reshaped from a 5x5 matrix of S_i*S_j
    train_states = train_states.reshape((shape[0], shape[1] * shape[2]))

    # needed for training the data: computing the outer product of S_i
    test_states = np.einsum("...i,...j->...ij", test, test)
    shape = test_states.shape

    # each row is a reshaped from a 5x5 matrix of S_i*S_j
    test_states = test_states.reshape((shape[0], shape[1] * shape[2]))

    l = np.logspace(-5, 6, 100)
    l = np.round(l, 5)
    i = 0

    # obtain score relatn with varying regularisation parameter lambda
    for a in l:
        i += 1

        # training our dataset
        lasso.set_params(alpha=a)
        lasso.fit(train_states, train_label)
        J = lasso.coef_.reshape((shape[1], shape[2]))

        mark1 = lasso.score(train_states, train_label) * 100
        mark2 = lasso.score(test_states, test_label) * 100

        score_tr.append(mark1)
        score_te.append(mark2)

    plt.figure()
    plt.grid()
    plt.title(r"Variation of train and test score. n: %s" % n)
    plt.xlabel(r"log($\lambda$)")
    plt.ylabel(r"$R^{2}$ (in %)")
    plt.plot(np.log(l), score_tr, label="Training Score")
    plt.plot(np.log(l), score_te, label="Testing Score")
    plt.legend()
    plt.savefig(r"%s.png" % n)
\end{minted}

\subsection{1D training using Ridge regression}

\begin{minted}{python}
import numpy as np
import matplotlib.pyplot as plt
from sklearn import linear_model

# function to return the energy of our lattice
def energy(l):
    e = 0
    j = 1
    # our energy assumes periodic boundary conditions
    for i in range(l.size - 1):

        e += -j * l[i] * (l[i - 1] + l[i + 1])

    e += -j * l[l.size - 1] * (l[l.size - 2] + l[0])

    return e


# length of our lattice
d = 50
# no of 1d ising models
n = 10000
score_tr = []
score_te = []

# defining our lattice
l = np.random.choice([-1, 1], size=(n, d))

# array to store the labels
label = np.zeros(n)

# creating the labels
for i in range(n):
    label[i] = energy(l[i])

# partition of our training n testing sets
m = int(7 * n / 10)

train = l[:m]
test = l[m:]

train_label = label[:m]
test_label = label[m:]

# data generated!!

# setting up our ML algos
leastsq = linear_model.LinearRegression()
ridge = linear_model.Ridge(tol=0.01)
lasso = linear_model.Lasso()

# needed for training the data: computing the outer product of S_i
train_states = np.einsum("...i,...j->...ij", train, train)
shape = train_states.shape

# each row is a reshaped from a 5x5 matrix of S_i*S_j
train_states = train_states.reshape((shape[0], shape[1] * shape[2]))

# needed for training the data: computing the outer product of S_i
test_states = np.einsum("...i,...j->...ij", test, test)
shape = test_states.shape

# each row is a reshaped from a 5x5 matrix of S_i*S_j
test_states = test_states.reshape((shape[0], shape[1] * shape[2]))

l = np.logspace(-5, 6, 100)
l = np.round(l, 5)
i = 0
for a in l:
    i += 1

    # training our dataset
    ridge.set_params(alpha=a)
    ridge.fit(train_states, train_label)
    J = ridge.coef_.reshape((shape[1], shape[2]))

    mark1 = ridge.score(train_states, train_label) * 100
    mark2 = ridge.score(test_states, test_label) * 100

    score_tr.append(mark1)
    score_te.append(mark2)

    plt.figure()
    plt.title(r"$\lambda$=%s" % a)
    c = plt.imshow(
        J, vmin=-1, vmax=1, cmap="Spectral_r", interpolation="nearest", origin="lower"
    )
    plt.colorbar(c)
    plt.savefig(r"%s.png" % i)

plt.figure()
plt.grid()
plt.title(r"Variation of train and test score. n: %s" % n)
plt.xlabel(r"$\lambda$")
plt.ylabel(r"$R^{2}$ (in %)")
plt.plot(np.log(l), score_tr, label="Training Score")
plt.plot(np.log(l), score_te, label="Testing Score")
plt.legend()
plt.show()
\end{minted}

\subsection{Implementation of 2nd nearest neighbour interaction for 1D Ising model}

This is only a snippet of code for implementing one-dimensional model where 2$^{nd}$ nearest neighbour interaction.

\begin{minted}{python}

# defining energy function for next to next atom interactions

# function to take care of periodic index of our lattice
def pbc(i, d):
    if i < d:
        return i

    else:
        return d - i


# function to return the energy of our lattice
def energy(l, d):
    e = 0
    # center is the atom, left n right r interactions with neighbours
    j = [-0.5, -1, 0, -1, -0.5]

    # our energy assumes periodic boundary conditions
    for i in range(l.size):

        e += l[i] * (
            j[pbc(0, d)] * l[pbc(i - 2, d)]
            + j[pbc(1, d)] * l[pbc(i - 1, d)]
            + j[pbc(2, d)] * l[pbc(i, d)]
            + j[pbc(3, d)] * l[pbc(i + 1, d)]
            + j[pbc(4, d)] * l[pbc(i + 2, d)]
        )

    return e
\end{minted}

\subsection{Generation of 2D Ising data using Metropolis}

\begin{minted}{python}
import numpy as np
import numba
import time

# define an ising model class to simplify calculations...
class ising_model_2D:
    # inputs our lattice, keeps the data of our lattice's energy and spin tagged
    def __init__(self, d, a, mfield):
        # lattice with a:(1-a) down-up
        temp = np.random.random((d, d))

        temp[temp >= a] = 1
        temp[temp < a] = -1

        # create 0 in the border (just to make neighbour atom calculations easier)
        temp = np.pad(array=temp, pad_width=1, mode="constant")
        self.lattice = temp.copy()
        del temp

        self.b = mfield

        # initialise the energy n net spin of our lattice
        self.energy = self.get_energy()  # use it to get monte carlo data.
        self.spin = np.sum(self.lattice)

    # to find the stable energy of our lattice
    def stable_energy(self):
        # the most stable energy(alligned spins))
        n = np.ones((d, d))
        n = np.pad(array=n, pad_width=1, mode="constant")

        e = 0
        # look at the i,jth atom.
        for i in range(1, d + 1):
            for j in range(1, d + 1):
                # see the neighbouring atoms(up,down,left,right)
                g = np.array([[0, 1, 0], [1, 0, 1], [0, 1, 0]])

                # calculate the sigma_ij*sigma_i'j' around our atom at ij
                m = -n[i, j] * g * n[i - 1 : i + 2, j - 1 : j + 2]

                # sum over all the neighbour sigmas
                e += np.sum(m)

        return e

    # fn to find the energy of our lattice with B
    def get_energy(self):
        e = 0
        # look at the i,jth atom.
        for i in range(1, d + 1):  # correction: d->d+1..!
            for j in range(1, d + 1):
                # see the neighbouring atoms(up,down,left,right)
                g = np.array([[0, 1, 0], [1, 0, 1], [0, 1, 0]])

                # calculate the sigma_ij*sigma_i'j' around our atom at ij
                m = -self.lattice[i, j] * g * self.lattice[i - 1 : i + 2, j - 1 : j + 2]

                # sum over all the neighbour sigmas
                e += np.sum(m) - self.b * self.lattice[i, j]

        return e

    # fn to calculate the energy of lattice on flipping spin at (x,y)
    def flipcheck(self, x, y):
        # the energy change comes only from the neighbours of the spin we flipped at (x,y)
        g = np.array([[0, 1, 0], [1, 0, 1], [0, 1, 0]])

        # calculate the sigma_ij*sigma_i'j' around our atom at ij
        m = -self.lattice[x, y] * g * self.lattice[x - 1 : x + 2, y - 1 : y + 2]

        # sum over all the neighbour sigmas and the effect of B
        e1 = np.sum(m) - self.lattice[x, y] * self.b

        # flip the (x,y)th spin
        self.lattice[x, y] *= -1

        # print('in flip fn', self.lattice)

        # calculate the sigma_ij*sigma_i'j' around our atom at ij
        m = -self.lattice[x, y] * g * self.lattice[x - 1 : x + 2, y - 1 : y + 2]

        # sum over all the neighbour sigmas and the effect of B
        e2 = np.sum(m) - self.lattice[x, y] * self.b

        # flip bacc the (x,y)th spin
        self.lattice[x, y] *= -1
        # print('after flip fn', self.lattice)
        return e1, e2

    # fn to flip the (x,y)th spin
    def flip(self, x, y):
        # spin at (x,y) flippd
        self.lattice[x, y] *= -1

        # update the energy of our system after the iterations...
        e1, e2 = self.flipcheck(x, y)

        # the energy difference is double counted for all the neighbouring bonds
        self.energy += 2 * (e1 - e2)

        # update spin
        self.spin += 2 * self.lattice[x, y]


# dimension of our array
d = 50
# optional addition of external magnetic field B
b = 0.0

# metropolis algorithm
@numba.njit("f8[:,:](f8[:,:], f8, i8)", nogil=True)
def metro(lattt, beta, reps):
    lat = lattt.copy()
    for i in range(reps):

        # random pick of our point in lattice
        x, y = np.random.randint(1, d), np.random.randint(1, d)

        # energies of our two systems
        # e1, e2 = flipcheck(lat, x, y)

        # manual spinflip check
        g = np.array([[0, 1, 0], [1, 0, 1], [0, 1, 0]])
        m = -lat[x, y] * g * lat[x - 1 : x + 2, y - 1 : y + 2]
        e1 = np.sum(m) - lat[x, y] * b
        lat[x, y] *= -1
        m = -lat[x, y] * g * lat[x - 1 : x + 2, y - 1 : y + 2]
        e2 = np.sum(m) - lat[x, y] * b
        lat[x, y] *= -1

        # if energy increase only keep with a probability(more energy increase, lesser the prob)
        if e2 > e1:
            # probability of flipping if unfavourable
            p = np.exp(-beta * (e2 - e1))

            if np.random.random() < p:

                lat[x, y] *= -1

        # flip the lattice spin if energy decreases
        else:
            lat[x, y] *= -1

    return lat


# temperature fineness
h = 20
t = np.linspace(0.25, 4, h)

master = []
for T in t:

    data = []
    print("T: %s" % T)
    print("Data generated:")

    # no of datasets in each temperature
    n = 10000
    mat = time.time()
    for i in range(n):
        print(i)
        start = time.time()
        lat = ising_model_2D(d, 0.5, b)

        lat.lattice = metro(lat.lattice, 1 / T, 60000)
        lat.energy = lat.get_energy()
        lat.spin = np.sum(lat.lattice)

        data.append(lat)
        # plt.imshow(lat.lattice, cmap="Greys_r", interpolation="nearest", origin="lower")
        print("Execution time: ", time.time() - start)
    # save data in a file! Gets destroyed every iteration!!!
    data = np.array(data)
    master.append(data)
    print("Total time for beta: ", time.time() - mat, "\n")

master = np.array(master)
np.save("2d_ising_data_2", master, allow_pickle=True)
np.savetxt("Temperature.csv", t, delimiter=",")
\end{minted}

\subsection{Finding average energy and heat capacity of the generated 2D lattices}

\begin{minted}{python}

# 2d ising data load
data = np.load("2d_ising_data_2.npy", allow_pickle=true)

# temperature label load
t = np.genfromtxt("temperature.csv", delimiter=",")

# to see the statistical results of our generated data
e, spin = np.zeros(20), np.zeros(20)

print("size of temperature data: ", t.size, "\n")
print(
    "size of 2d ising model data: ",
    data.shape,
    "\n no.of temperature samples taken: ",
    data.shape[0],
    "\n no.of lattice per temperature sample: ",
    data.shape[1],
)

# plot the energy specific heat graffs
h = t.size
n = data.shape[1]

for i in range(h):
    avg_e = 0
    avg_spin = 0

    for j in range(n):
        avg_e += data[i][j].energy
        avg_spin += data[i][j].spin
    avg_e /= n
    avg_spin /= n

    e[i] = avg_e
    spin[i] = avg_spin


plt.figure()
plt.grid()
plt.title("avg. energy vs temperature")
plt.xlabel("t (units)")
plt.ylabel("e (units)")
plt.plot(t, e)
plt.plot(t, e, "k .")
plt.show()

c = np.zeros(h)
r = 1

for i in range(r, h - r):
    c[i] = (e[i + r] - e[i - r]) / (t[i + r] - t[i - r])

plt.figure()
plt.grid()
plt.title("heat capacity of lattice vs temperature")
plt.xlabel("t (units)")
plt.ylabel("de/dt (units)")
plt.plot(t[r : h - r], c[r : h - r])
plt.plot(t[r : h - r], c[r : h - r], "k .")
plt.show()
\end{minted}

\subsection{Training 2D Ising data using Random Forests}

\begin{minted}{python}
import numpy as np
import matplotlib.pyplot as plt
import time

# 2d ising data load
data = np.load("2d_ising_data_2.npy", allow_pickle=True)

# temperature label load
t = np.genfromtxt("temperature.csv", delimiter=",")

for i in range(20):
    for j in range(10000):
        data[i][j].lattice = data[i][j].lattice[1:-1, 1:-1].flatten()

data_ordered = data[:8]
data_critical = data[8:14]
data_disordered = data[14:]

# our training data
X_ordered = []
for i in range(len(data_ordered[:])):
    # lattice very close to critical temperature
    if i == len(data_ordered[:]) - 1:
        for j in range(data_ordered[0].size):
            X_ordered.append(data_ordered[i][j].lattice)
            X_ordered.append(data_ordered[i][j].lattice)
            X_ordered.append(data_ordered[i][j].lattice)
    # lattice bit near to critical temperature
    elif i == len(data_ordered[:]) - 2:
        for j in range(data_ordered[0].size):
            X_ordered.append(data_ordered[i][j].lattice)
            X_ordered.append(data_ordered[i][j].lattice)
    # lattice bit far from critical temperature
    else:
        for j in range(data_ordered[0].size):
            X_ordered.append(data_ordered[i][j].lattice)

X_disordered = []
for i in range(len(data_disordered[:])):
    # lattice very close to critical temperature
    if i == len(data_disordered[:]) - 1:
        for j in range(data_disordered[0].size):
            X_disordered.append(data_disordered[i][j].lattice)
            X_disordered.append(data_disordered[i][j].lattice)
            X_disordered.append(data_disordered[i][j].lattice)
    # lattice bit near to critical temperature
    elif i == len(data_disordered[:]) - 2:
        for j in range(data_disordered[0].size):
            X_disordered.append(data_disordered[i][j].lattice)
            X_disordered.append(data_disordered[i][j].lattice)
    # lattice bit far from critical temperature
    else:
        for j in range(data_disordered[0].size):
            X_disordered.append(data_disordered[i][j].lattice)

X_critical = []
for i in range(len(data_critical[:])):
    for j in range(data_critical[0].size):
        X_critical.append(data_critical[i][j].lattice)

del data, data_ordered, data_disordered, data_critical

# data labels for our data
y_ordered, y_critical, y_disordered = [], [], []

for i in range(len(X_ordered)):
    y_ordered.append(1)

for i in range(len(X_disordered)):
    y_disordered.append(0)

for k in range(len(X_critical) // 2):
    y_critical.append(1)

for k in range(len(X_critical) - len(X_critical) // 2):
    y_critical.append(0)

X = np.concatenate((X_ordered, X_disordered))
y = np.concatenate((y_ordered, y_disordered))

from sklearn.model_selection import train_test_split

train_to_test_ratio = 0.8

X_train, X_test, y_train, y_test = train_test_split(
    X, y, train_size=train_to_test_ratio, test_size=1.0 - train_to_test_ratio
)

print("X_train shape:", X_train.shape)
print("Y_train shape:", y_train.shape)
print()
print(X_train.shape[0], "train samples")
print(len(X_critical), "critical samples")
print(X_test.shape[0], "test samples")

# Training model
from sklearn.ensemble import RandomForestClassifier

min_estimators = 10
max_estimators = 101
classifer = RandomForestClassifier

n_estimator_range = np.arange(min_estimators, max_estimators, 10)
leaf_size_list = [10, 2500, 5000, 7500, 10000]

m = len(n_estimator_range)
n = len(leaf_size_list)

RFC_OOB_accuracy = np.zeros((n, m))
RFC_train_accuracy = np.zeros((n, m))
RFC_test_accuracy = np.zeros((n, m))
RFC_critical_accuracy = np.zeros((n, m))
run_time = np.zeros((n, m))

print_flag = True

for i, leaf_size in enumerate(leaf_size_list):
    # Define Random Forest Classifier
    myRF_clf = classifer(
        n_estimators=min_estimators,
        max_depth=None,
        min_samples_split=leaf_size,  # minimum number of sample per leaf
        oob_score=True,
        random_state=0,
        warm_start=True,  # this ensures that you add estimators without retraining everything
    )
    for j, n_estimator in enumerate(n_estimator_range):

        print("n_estimators: %i, leaf_size: %i" % (n_estimator, leaf_size))

        start_time = time.time()
        myRF_clf.set_params(n_estimators=n_estimator)
        myRF_clf.fit(X_train, y_train)
        run_time[i, j] = time.time() - start_time

        # check accuracy
        RFC_train_accuracy[i, j] = myRF_clf.score(X_train, y_train)
        RFC_OOB_accuracy[i, j] = myRF_clf.oob_score_
        RFC_test_accuracy[i, j] = myRF_clf.score(X_test, y_test)
        RFC_critical_accuracy[i, j] = myRF_clf.score(X_critical, y_critical)
        if print_flag:
            result = (
                run_time[i, j],
                RFC_train_accuracy[i, j],
                RFC_OOB_accuracy[i, j],
                RFC_test_accuracy[i, j],
                RFC_critical_accuracy[i, j],
            )
            print(
                "{0:<15}{1:<15}{2:<15}{3:<15}{4:<15}".format(
                    "time (s)",
                    "train score",
                    "OOB estimate",
                    "test score",
                    "critical score",
                )
            )
            print("{0:<15.4f}{1:<15.4f}{2:<15.4f}{3:<15.4f}{4:<15.4f}".format(*result))

plt.figure()
plt.title("Training Score")
plt.grid()
for i in range(n):
    plt.plot(
        n_estimator_range,
        RFC_train_accuracy[i],
        "--",
        label="Samples each node-%s" % leaf_size_list[i],
    )
    plt.plot(n_estimator_range, RFC_train_accuracy[i], ".k")


plt.xlabel("$N_\mathrm{estimators}$")
plt.ylabel("Accuracy")
lgd = plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)

plt.show()

plt.figure()
plt.title("Testing Score")
plt.grid()
for i in range(n):
    plt.plot(
        n_estimator_range,
        RFC_test_accuracy[i],
        "--",
        label="Samples each node-%s" % leaf_size_list[i],
    )
    plt.plot(n_estimator_range, RFC_test_accuracy[i], ".k")

plt.xlabel("$N_\mathrm{estimators}$")
plt.ylabel("Accuracy")
lgd = plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)

plt.show()

plt.figure()
plt.title("Critical Score")
plt.grid()
for i in range(n):
    plt.plot(
        n_estimator_range,
        RFC_critical_accuracy[i],
        "--",
        label="Samples each node-%s" % leaf_size_list[i],
    )
    plt.plot(n_estimator_range, RFC_critical_accuracy[i], ".k")


plt.xlabel("$N_\mathrm{estimators}$")
plt.ylabel("Accuracy")
lgd = plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)

plt.show()

\end{minted}

\twocolumn
